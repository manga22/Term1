# -*- coding: utf-8 -*-
"""CMOAssignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wYNilFayyJssmVG3usMjIPWCbw14PQpj
"""

import dill
import pickle
import numpy as np

#Q5 part a 
fx = dill.loads(pickle.load(open('f5.pkl','rb')))
grad = dill.loads(pickle.load(open('grad_f5.pkl','rb')))
hess = dill.loads(pickle.load(open('hess_f5.pkl','rb')))

#Q5 part b
b = np.array([0.76, 0.08, 1.12, 0.68])
Q = np.array([[0.78, -0.02, -0.12, -0.14],[-0.02,0.86,-0.04,0.06],[-0.12,-0.04,0.72,-0.08],[-0.14,0.06,-0.08,0.74]])

def hess():
  return Q
def grad(x):
  return np.matmul(hess(),x) - b
def fx(x):
  return 0.5 * np.sum(x * np.matmul(hess(),x)) - np.sum(b*x)

def l2_norm(x):
    x = np.array(x)
    return np.sqrt(np.sum(x * x))
'''
def grad_descent(X0, alpha=None, eps=None, method='Gradient descent'):
    X=X0
    grad_x=np.array(grad(X))
    k=0
    while(l2_norm(grad_x)>eps):
        k+=1
        alpha = np.sum(grad_x*grad_x)/(np.sum(grad_x * np.matmul(hess(),grad_x)))
        X=X-alpha*grad_x
        grad_x=np.array(grad(X))
        print(f'iter {k} : f(x)={fx(X)} grad(x)={l2_norm(grad(X))}')
    print(f'final f(x) at {k}th iter : {fx(X)}')
    return k

#X0 = np.array([0, 10])   #Q5 Part a
X0 = np.array([0,0,0,0])  #Q5 Part b
k = grad_descent(X0, eps = 10**(-6))

#Verify result
x_opt = np.matmul(np.linalg.inv(hess()), b)
print(f'X_true = {x_opt}')
print(f'f(X_true) = {fx(x_opt)}')
'''
"""# Q6 Inexact line search """

#Q6 Backtracking inexact line search
fx = dill.loads(pickle.load(open('f6.pkl','rb')))
grad = dill.loads(pickle.load(open('grad_f6.pkl','rb')))

def backtracking(X0, alpha=0.5, beta=0.5, eps=10**(-7)):
    X = X0
    u = np.array(grad(X))
    t=1
    k=0
    count_grad = 1
    count_f = 0
    print(t, grad(X), l2_norm(t*u))
    while(l2_norm(t*u)>eps):
      u= -np.array(grad(X))
      count_grad+=1
      f_xk = fx(X)
      count_f+=1
      t= 1
      while(fx(X+t*u)>(f_xk-alpha * t* np.sum(u*u))):
        count_f += 1
        t = t*beta
      X = X + t*u
      k+=1
      print(f'iter {k} : f(x)={fx(X)} tol={l2_norm(t*u)}')
    print(f'final f(x) at {k}th iter : {fx(X)}')
    print(f'f(X) is called {count_f} times')
    print(f'grad(X) is called {count_grad} times')
    return [k, X, fx(X)]

X0 = np.array([10,100,100,10])
print('Q6 Backtracking')
k, X, f_x = backtracking(X0, alpha=0.5, beta=0.5, eps=10**(-7))
print(k, X, fx(X))
print('Q6 Backtracking over')

"""# Q7 Golden section search and Fibonacci search"""
'''
def foo(x):
  return np.exp(-x)-np.cos(x)

def GS_Search(a, b, eps):
  xl = a
  xu = b
  k = 0
  l = 0.5* (1+ np.sqrt(5))
  r = l/(1+l)
  tol = 0.5*(xu - xl)
  while(tol>eps):
    d = (xu - xl)*r
    xn = xu - d
    xp = xl + d
    if(foo(xn)<foo(xp)):
      xu = xp
    else:
      xl = xn
    k+=2
    tol = 0.5*(xu - xl)
  return [0.5*(xu+xl), tol, k]
    
def Fibo_Search(a, b, N):
  xl = a
  xu = b
  k = 0
  f = np.zeros((N+1))
  f[0]=1
  f[1]=1
  for j in range(2,N+1):
    f[j]=f[j-1]+f[j-2]

  for  i in range(1,N):
    #print(i,N-i,N-i+1)
    r = f[N-i]/f[N-i+1]
    d = (xu - xl)*r
    xn = xu - d
    xp = xl + d
    if(foo(xn)<foo(xp)):
      xu = xp
    else:
      xl = xn
    k+=2
    tol = 0.5*(xu - xl)
  return [0.5*(xu+xl), tol, k]

x_opt, tol, nsc_20 = Fibo_Search(0,1,20)
print(f'Fibo N= 20: {x_opt, tol, nsc_20}')
x_opt, tol, nsc = GS_Search(0,1,tol)
print(f'GS for tol from N=20: {x_opt, tol, nsc}')

x_opt, tol, nsc_10 = Fibo_Search(0,1,10)
print(f'Fibo N= 10: {x_opt, tol, nsc_10}')
x_opt, tol, nsc = GS_Search(0,1,tol)
print(f'GS for tol from N=10: {x_opt, tol, nsc}')
'''